{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üß™ Module-02: Challenge Lab"
      ],
      "metadata": {
        "id": "PCEkQ54hWy0i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß™ Challenge Lab: Advanced Stop Word Removal & Word Frequencies\n",
        "\n",
        "üéØ Learning Goals\n",
        "* By the end of this lab, you will be able to:\n",
        "* Clean text (lowercase + remove punctuation)\n",
        "* Tokenize text into words\n",
        "* Remove stop words using NLTK and a custom list\n",
        "* Wrap logic into simple Python functions\n",
        "* Compute basic word frequency counts before and after stop word removal\n",
        "\n",
        "üß± Students must complete the missing sections marked as TODO\n",
        "\n",
        "‚úÖ  When they complete and run the code, they should see:\n",
        "*  A cleaned version of the text (lowercase, no punctuation)\n",
        "*  A list of all tokens (including ‚Äúthe‚Äù, ‚Äúis‚Äù, etc.)\n",
        "*  A list of filtered tokens with stop words and some custom words removedThe\n",
        "* Top 5 most frequent words before and after stop word removal"
      ],
      "metadata": {
        "id": "FhGy5Aue-bun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "# ------------------------------\n",
        "# 1. DOWNLOAD REQUIRED RESOURCES\n",
        "# ------------------------------\n",
        "\n",
        "# TODO: Download the NLTK stopwords once on a new computer\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# ------------------------------\n",
        "# 2. SAMPLE TEXT\n",
        "# ------------------------------\n",
        "\n",
        "text = \"\"\"\n",
        "Natural Language Processing enables computers to understand human language.\n",
        "It is used in chatbots, search engines, translation tools, and many other applications.\n",
        "Language processing can be challenging because human language is complex and full of nuance.\n",
        "\"\"\"\n",
        "\n",
        "# ------------------------------\n",
        "# 3. TEXT CLEANING FUNCTION\n",
        "# ------------------------------\n",
        "\n",
        "def clean_text(raw_text):\n",
        "    \"\"\"\n",
        "    Convert text to lowercase and remove punctuation.\n",
        "    Returns a cleaned string.\n",
        "    \"\"\"\n",
        "    # TODO: Convert to lowercase\n",
        "    lower_text = raw_text.lower()\n",
        "\n",
        "    # TODO: Remove punctuation using str.translate and string.punctuation\n",
        "    cleaned_text = lower_text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "# ------------------------------\n",
        "# 4. TOKENIZATION FUNCTION\n",
        "# ------------------------------\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Split text into a list of word tokens using spaces.\n",
        "    \"\"\"\n",
        "    # TODO: Split the text on spaces\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "# ------------------------------\n",
        "# 5. STOP WORD REMOVAL FUNCTION\n",
        "# ------------------------------\n",
        "\n",
        "def remove_stop_words(tokens):\n",
        "    \"\"\"\n",
        "    Remove NLTK English stop words AND some custom stop words.\n",
        "    Returns a new list of filtered tokens.\n",
        "    \"\"\"\n",
        "    # Get standard English stop words from NLTK\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # TODO: Add your own custom stop words here (e.g., common domain words)\n",
        "    custom_stop_words = {\"language\", \"processing\"}\n",
        "\n",
        "    # Combine both sets\n",
        "    all_stop_words = stop_words.union(custom_stop_words)\n",
        "\n",
        "    # TODO: Keep only the tokens that are NOT in all_stop_words\n",
        "    filtered = [word for word in tokens if word.lower() not in all_stop_words]\n",
        "\n",
        "    return filtered\n",
        "\n",
        "# ------------------------------\n",
        "# 6. FREQUENCY COUNT FUNCTION\n",
        "# ------------------------------\n",
        "\n",
        "def count_frequencies(tokens):\n",
        "    \"\"\"\n",
        "    Return a dictionary-like object with word counts.\n",
        "    \"\"\"\n",
        "    # TODO: Use Counter to count how many times each word appears\n",
        "    freq = Counter(tokens)\n",
        "    return freq\n",
        "\n",
        "# ------------------------------\n",
        "# 7. MAIN LOGIC\n",
        "# ------------------------------\n",
        "\n",
        "# Step 1: Clean the raw text\n",
        "cleaned_text = clean_text(text)\n",
        "\n",
        "# Step 2: Tokenize before stop word removal\n",
        "original_tokens = tokenize(cleaned_text)\n",
        "\n",
        "# Step 3: Remove stop words (standard + custom)\n",
        "filtered_tokens = remove_stop_words(original_tokens)\n",
        "\n",
        "# Step 4: Count frequencies before and after\n",
        "original_freq = count_frequencies(original_tokens)\n",
        "filtered_freq = count_frequencies(filtered_tokens)\n",
        "\n",
        "# ------------------------------\n",
        "# 8. PRINT RESULTS\n",
        "# ------------------------------\n",
        "\n",
        "print(\"=== CLEANED TEXT ===\")\n",
        "print(cleaned_text)\n",
        "\n",
        "print(\"\\n=== ORIGINAL TOKENS (with stop words) ===\")\n",
        "print(original_tokens)\n",
        "\n",
        "print(\"\\n=== FILTERED TOKENS (stop words removed) ===\")\n",
        "print(filtered_tokens)\n",
        "\n",
        "print(\"\\n=== TOP 5 WORDS BEFORE STOP WORD REMOVAL ===\")\n",
        "print(original_freq.most_common(5))\n",
        "\n",
        "print(\"\\n=== TOP 5 WORDS AFTER STOP WORD REMOVAL ===\")\n",
        "print(filtered_freq.most_common(5))\n",
        "\n"
      ],
      "metadata": {
        "id": "nMOJ2feV9ION",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95053a5e-6cb3-4181-b9c2-c6642bac120c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CLEANED TEXT ===\n",
            "\n",
            "natural language processing enables computers to understand human language\n",
            "it is used in chatbots search engines translation tools and many other applications\n",
            "language processing can be challenging because human language is complex and full of nuance\n",
            "\n",
            "\n",
            "=== ORIGINAL TOKENS (with stop words) ===\n",
            "['natural', 'language', 'processing', 'enables', 'computers', 'to', 'understand', 'human', 'language', 'it', 'is', 'used', 'in', 'chatbots', 'search', 'engines', 'translation', 'tools', 'and', 'many', 'other', 'applications', 'language', 'processing', 'can', 'be', 'challenging', 'because', 'human', 'language', 'is', 'complex', 'and', 'full', 'of', 'nuance']\n",
            "\n",
            "=== FILTERED TOKENS (stop words removed) ===\n",
            "['natural', 'enables', 'computers', 'understand', 'human', 'used', 'chatbots', 'search', 'engines', 'translation', 'tools', 'many', 'applications', 'challenging', 'human', 'complex', 'full', 'nuance']\n",
            "\n",
            "=== TOP 5 WORDS BEFORE STOP WORD REMOVAL ===\n",
            "[('language', 4), ('processing', 2), ('human', 2), ('is', 2), ('and', 2)]\n",
            "\n",
            "=== TOP 5 WORDS AFTER STOP WORD REMOVAL ===\n",
            "[('human', 2), ('natural', 1), ('enables', 1), ('computers', 1), ('understand', 1)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    }
  ]
}